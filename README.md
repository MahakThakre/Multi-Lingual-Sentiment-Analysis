# ðŸ§  Multilingual Sentiment Analysis (NPPE-1 Competition)

## ðŸš€ Overview
This project is developed as part of the **NPPE-1 Multilingual Sentiment Analysis Competition** hosted on Kaggle. The goal is to fine-tune the **LLaMA 3.1-8B-Instruct** model to classify text sentiment (Positive or Negative) across **13 Indian languages** using **LoRA-based parameter-efficient fine-tuning**, all within **Kaggle Notebook constraints**.

## ðŸ Competition Timeline
- **Start:** February 14, 2025  
- **End:** February 18, 2025  
- **Evaluation Metric:** F1 Score  
- **Restrictions:**  
  - Must use **LLaMA 3.1-8B-Instruct**  
  - Training only on **provided dataset**  
  - **No external data or local training allowed**  
  - **Kaggle Notebooks only**

## ðŸ› ï¸ Technologies & Tools
- LLaMA 3.1-8B-Instruct
- Unsloth (https://github.com/unslothai/unsloth)
- LoRA (Low-Rank Adaptation)
- Hugging Face Transformers
- Datasets
- Pandas
- Scikit-learn
- Kaggle Notebook

## ðŸ§¾ Dataset
- **Train File:** `train.csv`  
- **Test File:** `test.csv`  
- **Languages Covered:**
  - Assamese, Bodo, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu, Urdu

## ðŸ§ª Methodology

### âœ… Fine-Tuning Strategy
- **Model Loading:** Loaded the base LLaMA 3.1-8B-Instruct model with 4-bit quantization.
- **LoRA Config:**
  - Rank (`r`): 16
  - LoRA Alpha: 16
  - Target Modules: QKV, MLP
  - Dropout: 0
  - Gradient Checkpointing: Enabled via `"unsloth"` for memory efficiency.
- **Prompt Template:** Custom Alpaca-style instruction format per sample.

### ðŸ”„ Data Handling
- Labeled sentences mapped as `1` for Positive and `0` for Negative.
- Mapped language codes to full names for clarity in prompts.
- Used 80/20 train-validation split.

## ðŸ§  Training Configuration
- **Sequence Length:** 4096
- **Batch Size:** Auto-managed by Unsloth for optimal GPU usage
- **Evaluation Strategy:** F1 Score on validation set
- **Trainer:** `SFTTrainer` from TRL for supervised fine-tuning

## ðŸ“Š Evaluation
- **Metric:** F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
- **Grading Policy:**
  - For `F1 Score â‰¤ 0.5`: `Score = 2 * F1 * 50`
  - For `F1 Score > 0.5`: `Score = ((F1 - 0.5) / (MaxF1 - 0.5)) * 50 + 50`

## ðŸ“‚ Directory Structure
```
.
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”œâ”€â”€ llama_finetune_sentiment.ipynb  # Main notebook
â”œâ”€â”€ README.md
```

## ðŸ“„ License
Released under the Apache 2.0 License.
