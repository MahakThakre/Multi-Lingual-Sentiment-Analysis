{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102},{"sourceId":81881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:19:20.676335Z","iopub.execute_input":"2025-02-16T17:19:20.676638Z","iopub.status.idle":"2025-02-16T17:19:21.000401Z","shell.execute_reply.started":"2025-02-16T17:19:20.676608Z","shell.execute_reply":"2025-02-16T17:19:20.999566Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install unsloth\n!pip install accelerate\n!pip install peft\n!pip install torch==2.1.2 --force-reinstall\n!pip install --upgrade transformers\n!pip install --upgrade unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:19:21.001461Z","iopub.execute_input":"2025-02-16T17:19:21.001888Z","iopub.status.idle":"2025-02-16T17:26:27.856723Z","shell.execute_reply.started":"2025-02-16T17:19:21.001850Z","shell.execute_reply":"2025-02-16T17:26:27.855734Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch._dynamo\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:27.860640Z","iopub.execute_input":"2025-02-16T17:26:27.860895Z","iopub.status.idle":"2025-02-16T17:26:30.789819Z","shell.execute_reply.started":"2025-02-16T17:26:27.860869Z","shell.execute_reply":"2025-02-16T17:26:30.789107Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datetime import datetime\nfrom unsloth import FastLanguageModel\nimport torch\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:30.790953Z","iopub.execute_input":"2025-02-16T17:26:30.791325Z","iopub.status.idle":"2025-02-16T17:26:55.929315Z","shell.execute_reply.started":"2025-02-16T17:26:30.791302Z","shell.execute_reply":"2025-02-16T17:26:55.928637Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\ntest_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nmap_dict = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_path,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:55.981381Z","iopub.execute_input":"2025-02-16T17:26:55.981650Z","iopub.status.idle":"2025-02-16T17:28:19.344274Z","shell.execute_reply.started":"2025-02-16T17:26:55.981611Z","shell.execute_reply":"2025-02-16T17:28:19.343468Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2bd435947e43e08c69819b6194f367"}},"metadata":{}},{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2 does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Adding LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:19.346527Z","iopub.execute_input":"2025-02-16T17:28:19.346752Z","iopub.status.idle":"2025-02-16T17:28:25.767186Z","shell.execute_reply.started":"2025-02-16T17:28:19.346733Z","shell.execute_reply":"2025-02-16T17:28:25.766435Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the {} language.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    texts = []\n    new_labels = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    labels = examples['label']\n    for sent, lang, lab in zip(sentences, langs, labels):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        #text = generate_prompt(example) + EOS_TOKEN\n        new_lab = 1 if lab == 'Positive' else 0\n        text = alpaca_prompt.format(map_dict[lang], sent, new_lab) + EOS_TOKEN\n        texts.append(text)\n        new_labels.append(new_lab)\n    return { \"text\" : texts, \"label\": new_labels}\n\ndataset = load_dataset(\"csv\", data_files = [file_path])[\"train\"]\ndataset = dataset.map(formatting_prompts_func, batched = True,)\nsplit_dataset = dataset.train_test_split(test_size = 0.2, seed = 62)\ntrain_dataset, valid_dataset = split_dataset['train'], split_dataset['test']\ntrain_dataset, valid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:25.806559Z","iopub.execute_input":"2025-02-16T17:28:25.806832Z","iopub.status.idle":"2025-02-16T17:28:26.121308Z","shell.execute_reply.started":"2025-02-16T17:28:25.806802Z","shell.execute_reply":"2025-02-16T17:28:26.120597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b878b5b7bcb439cac4f21430d917e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822fed378b5f4d56a05ae3cdd435df21"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 800\n }),\n Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 200\n }))"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_dataset[0], valid_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:26.122096Z","iopub.execute_input":"2025-02-16T17:28:26.122340Z","iopub.status.idle":"2025-02-16T17:28:26.128633Z","shell.execute_reply.started":"2025-02-16T17:28:26.122321Z","shell.execute_reply":"2025-02-16T17:28:26.127815Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"({'ID': 150,\n  'sentence': '‡¨∏‡¨æ‡¨Æ‡≠ç‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï ‡¨∏‡¨™‡≠ç‡¨§‡¨æ‡¨π‡¨ó‡≠Å‡¨°‡¨ø‡¨ï‡¨∞‡≠á, ‡¨∏‡≠ã‡¨®‡¨ø‡¨≤‡¨ø‡¨≠‡≠ç ‡¨ï‡¨ø‡¨õ‡¨ø ‡¨¨‡¨æ‡¨∏‡≠ç‡¨§‡¨¨‡¨∞‡≠á ‡¨≤‡≠ã‡¨ï‡¨™‡≠ç‡¨∞‡¨ø‡≠ü ‡¨∏‡¨ø‡¨∞‡¨ø‡¨ú‡≠ç ‡¨è‡¨¨‡¨Ç ‡¨≤‡¨ó‡¨æ‡¨§‡¨æ‡¨∞ ‡¨¶‡≠á‡¨ñ‡¨ø‡¨¨‡¨æ ‡¨™‡¨æ‡¨á‡¨Å ‡¨ï‡¨ø‡¨õ‡¨ø ‡¨≠‡¨≤ ‡¨¨‡¨ø‡¨∑‡≠ü‡¨¨‡¨∏‡≠ç‡¨§‡≠Å ‡¨§‡¨æ‡¨≤‡¨ø‡¨ï‡¨æ‡¨≠‡≠Å‡¨ï‡≠ç‡¨§ ‡¨ï‡¨∞‡≠Å‡¨õ‡¨ø |',\n  'label': 1,\n  'language': 'or',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Odia language.\\n\\n### Input:\\n‡¨∏‡¨æ‡¨Æ‡≠ç‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï ‡¨∏‡¨™‡≠ç‡¨§‡¨æ‡¨π‡¨ó‡≠Å‡¨°‡¨ø‡¨ï‡¨∞‡≠á, ‡¨∏‡≠ã‡¨®‡¨ø‡¨≤‡¨ø‡¨≠‡≠ç ‡¨ï‡¨ø‡¨õ‡¨ø ‡¨¨‡¨æ‡¨∏‡≠ç‡¨§‡¨¨‡¨∞‡≠á ‡¨≤‡≠ã‡¨ï‡¨™‡≠ç‡¨∞‡¨ø‡≠ü ‡¨∏‡¨ø‡¨∞‡¨ø‡¨ú‡≠ç ‡¨è‡¨¨‡¨Ç ‡¨≤‡¨ó‡¨æ‡¨§‡¨æ‡¨∞ ‡¨¶‡≠á‡¨ñ‡¨ø‡¨¨‡¨æ ‡¨™‡¨æ‡¨á‡¨Å ‡¨ï‡¨ø‡¨õ‡¨ø ‡¨≠‡¨≤ ‡¨¨‡¨ø‡¨∑‡≠ü‡¨¨‡¨∏‡≠ç‡¨§‡≠Å ‡¨§‡¨æ‡¨≤‡¨ø‡¨ï‡¨æ‡¨≠‡≠Å‡¨ï‡≠ç‡¨§ ‡¨ï‡¨∞‡≠Å‡¨õ‡¨ø |\\n\\n### Response:\\n1<|eot_id|>'},\n {'ID': 466,\n  'sentence': '‡§ï‡•ç‡§∞‡•ã‡§Æ‡§æ ‡§è‡§∏‡•Ä ‡§ï‡§æ ‡§è‡§Ü‡§à ‡§Æ‡•ã‡§°, ‡§ú‡§ø‡§∏‡•á ‡§ë‡§ü‡•ã ‡§ë‡§™‡§∞‡•á‡§∂‡§® ‡§Æ‡•ã‡§° ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ï‡§Æ‡§∞‡•á ‡§ï‡•á ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§™‡§Ç‡§ñ‡•á ‡§ï‡•Ä ‡§ó‡§§‡§ø ‡§î‡§∞ ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§ï‡•ã ‡§ë‡§ü‡•ã‡§Æ‡•á‡§ü‡§ø‡§ï‡§≤‡•Ä ‡§∏‡•á‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ö‡•Ç‡§Ç‡§ï‡§ø ‡§Ø‡§π ‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡•Ä‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏ ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§á‡§§‡§®‡•á ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç‡•§',\n  'label': 0,\n  'language': 'hi',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Hindi language.\\n\\n### Input:\\n‡§ï‡•ç‡§∞‡•ã‡§Æ‡§æ ‡§è‡§∏‡•Ä ‡§ï‡§æ ‡§è‡§Ü‡§à ‡§Æ‡•ã‡§°, ‡§ú‡§ø‡§∏‡•á ‡§ë‡§ü‡•ã ‡§ë‡§™‡§∞‡•á‡§∂‡§® ‡§Æ‡•ã‡§° ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ï‡§Æ‡§∞‡•á ‡§ï‡•á ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§™‡§Ç‡§ñ‡•á ‡§ï‡•Ä ‡§ó‡§§‡§ø ‡§î‡§∞ ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§ï‡•ã ‡§ë‡§ü‡•ã‡§Æ‡•á‡§ü‡§ø‡§ï‡§≤‡•Ä ‡§∏‡•á‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ö‡•Ç‡§Ç‡§ï‡§ø ‡§Ø‡§π ‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡•Ä‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏ ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§á‡§§‡§®‡•á ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç‡•§\\n\\n### Response:\\n0<|eot_id|>'})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n        warmup_steps = 5,\n        num_train_epochs = 5, # Set this for 1 full training run.\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:26.129271Z","iopub.execute_input":"2025-02-16T17:28:26.129465Z","iopub.status.idle":"2025-02-16T17:28:31.021493Z","shell.execute_reply.started":"2025-02-16T17:28:26.129449Z","shell.execute_reply":"2025-02-16T17:28:31.020437Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50cf9a15e4d64c2a97407b4ba9c66ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80f0047e8a84881a03a73f3a3fc6732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b099221c04d4c2f94f7020063b67eae"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:31.022651Z","iopub.execute_input":"2025-02-16T17:28:31.022916Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 800 | Num Epochs = 5\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 16 | Total steps = 250\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  8/250 01:53 < 1:16:27, 0.05 it/s, Epoch 0.14/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Inference Test\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nexample = train_dataset[0]\ninputs = tokenizer(\n[\n    alpaca_prompt.format(map_dict[example['language']], example['sentence'], '')\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nresult = tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0]\nresult, example['label']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ndef get_results(examples):\n    results = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(map_dict[langs], sentences, '')\n        ], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    result = int(tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0])\n    return {'result': result}\n\ntrain_dataset = train_dataset.map(get_results)\nvalid_dataset = valid_dataset.map(get_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_F1(actual, predicted):\n    tp, tn, fp, fn = 0, 0, 0, 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            if actual[i] == 1: tp += 1\n            else: tn += 1\n        else:\n            if actual[i] == 1: fn += 1\n            else: fp += 1\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    accuracy = (tp+tn) / (tp+tn+fp+fn)\n    f1_score = (2 * precision * recall) / (precision+recall)\n    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')\n    print(f'F1 Score: {f1_score}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Train Metrics')\ncalculate_F1(train_dataset[:]['label'], train_dataset[:]['result'])\nprint('Validation Metrics')\ncalculate_F1(valid_dataset[:]['label'], valid_dataset[:]['result'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files = [test_file_path])[\"train\"]\ntest_dataset = test_dataset.map(get_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test_dataset[:]['result']\npredictions = ['Positive' if predictions[i] == 1 else 'Negative' for i in range(len(predictions))]\nlen(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dl = pd.DataFrame({'ID': list(range(1,len(predictions)+1)), 'label': predictions})\ndl.to_csv('output.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}