{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102},{"sourceId":81881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:19:20.676335Z","iopub.execute_input":"2025-02-16T17:19:20.676638Z","iopub.status.idle":"2025-02-16T17:19:21.000401Z","shell.execute_reply.started":"2025-02-16T17:19:20.676608Z","shell.execute_reply":"2025-02-16T17:19:20.999566Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install unsloth\n!pip install accelerate\n!pip install peft\n!pip install torch==2.1.2 --force-reinstall\n!pip install --upgrade transformers\n!pip install --upgrade unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:19:21.001461Z","iopub.execute_input":"2025-02-16T17:19:21.001888Z","iopub.status.idle":"2025-02-16T17:26:27.856723Z","shell.execute_reply.started":"2025-02-16T17:19:21.001850Z","shell.execute_reply":"2025-02-16T17:26:27.855734Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch._dynamo\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:27.860640Z","iopub.execute_input":"2025-02-16T17:26:27.860895Z","iopub.status.idle":"2025-02-16T17:26:30.789819Z","shell.execute_reply.started":"2025-02-16T17:26:27.860869Z","shell.execute_reply":"2025-02-16T17:26:30.789107Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datetime import datetime\nfrom unsloth import FastLanguageModel\nimport torch\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:30.790953Z","iopub.execute_input":"2025-02-16T17:26:30.791325Z","iopub.status.idle":"2025-02-16T17:26:55.929315Z","shell.execute_reply.started":"2025-02-16T17:26:30.791302Z","shell.execute_reply":"2025-02-16T17:26:55.928637Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\ntest_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nmap_dict = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_path,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:55.981381Z","iopub.execute_input":"2025-02-16T17:26:55.981650Z","iopub.status.idle":"2025-02-16T17:28:19.344274Z","shell.execute_reply.started":"2025-02-16T17:26:55.981611Z","shell.execute_reply":"2025-02-16T17:28:19.343468Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2bd435947e43e08c69819b6194f367"}},"metadata":{}},{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2 does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Adding LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:19.346527Z","iopub.execute_input":"2025-02-16T17:28:19.346752Z","iopub.status.idle":"2025-02-16T17:28:25.767186Z","shell.execute_reply.started":"2025-02-16T17:28:19.346733Z","shell.execute_reply":"2025-02-16T17:28:25.766435Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the {} language.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    texts = []\n    new_labels = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    labels = examples['label']\n    for sent, lang, lab in zip(sentences, langs, labels):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        #text = generate_prompt(example) + EOS_TOKEN\n        new_lab = 1 if lab == 'Positive' else 0\n        text = alpaca_prompt.format(map_dict[lang], sent, new_lab) + EOS_TOKEN\n        texts.append(text)\n        new_labels.append(new_lab)\n    return { \"text\" : texts, \"label\": new_labels}\n\ndataset = load_dataset(\"csv\", data_files = [file_path])[\"train\"]\ndataset = dataset.map(formatting_prompts_func, batched = True,)\nsplit_dataset = dataset.train_test_split(test_size = 0.2, seed = 62)\ntrain_dataset, valid_dataset = split_dataset['train'], split_dataset['test']\ntrain_dataset, valid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:25.806559Z","iopub.execute_input":"2025-02-16T17:28:25.806832Z","iopub.status.idle":"2025-02-16T17:28:26.121308Z","shell.execute_reply.started":"2025-02-16T17:28:25.806802Z","shell.execute_reply":"2025-02-16T17:28:26.120597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b878b5b7bcb439cac4f21430d917e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822fed378b5f4d56a05ae3cdd435df21"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 800\n }),\n Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 200\n }))"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_dataset[0], valid_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:26.122096Z","iopub.execute_input":"2025-02-16T17:28:26.122340Z","iopub.status.idle":"2025-02-16T17:28:26.128633Z","shell.execute_reply.started":"2025-02-16T17:28:26.122321Z","shell.execute_reply":"2025-02-16T17:28:26.127815Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"({'ID': 150,\n  'sentence': 'ସାମ୍ପ୍ରତିକ ସପ୍ତାହଗୁଡିକରେ, ସୋନିଲିଭ୍ କିଛି ବାସ୍ତବରେ ଲୋକପ୍ରିୟ ସିରିଜ୍ ଏବଂ ଲଗାତାର ଦେଖିବା ପାଇଁ କିଛି ଭଲ ବିଷୟବସ୍ତୁ ତାଲିକାଭୁକ୍ତ କରୁଛି |',\n  'label': 1,\n  'language': 'or',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Odia language.\\n\\n### Input:\\nସାମ୍ପ୍ରତିକ ସପ୍ତାହଗୁଡିକରେ, ସୋନିଲିଭ୍ କିଛି ବାସ୍ତବରେ ଲୋକପ୍ରିୟ ସିରିଜ୍ ଏବଂ ଲଗାତାର ଦେଖିବା ପାଇଁ କିଛି ଭଲ ବିଷୟବସ୍ତୁ ତାଲିକାଭୁକ୍ତ କରୁଛି |\\n\\n### Response:\\n1<|eot_id|>'},\n {'ID': 466,\n  'sentence': 'क्रोमा एसी का एआई मोड, जिसे ऑटो ऑपरेशन मोड के रूप में भी जाना जाता है, कमरे के तापमान के आधार पर पंखे की गति और तापमान को ऑटोमेटिकली सेट करता है। चूंकि यह आर्टिफीशियल इंटेलिजेंस पर काम करता है, इसलिए परिणाम इतने अच्छे नहीं हैं।',\n  'label': 0,\n  'language': 'hi',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Hindi language.\\n\\n### Input:\\nक्रोमा एसी का एआई मोड, जिसे ऑटो ऑपरेशन मोड के रूप में भी जाना जाता है, कमरे के तापमान के आधार पर पंखे की गति और तापमान को ऑटोमेटिकली सेट करता है। चूंकि यह आर्टिफीशियल इंटेलिजेंस पर काम करता है, इसलिए परिणाम इतने अच्छे नहीं हैं।\\n\\n### Response:\\n0<|eot_id|>'})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n        warmup_steps = 5,\n        num_train_epochs = 5, # Set this for 1 full training run.\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:26.129271Z","iopub.execute_input":"2025-02-16T17:28:26.129465Z","iopub.status.idle":"2025-02-16T17:28:31.021493Z","shell.execute_reply.started":"2025-02-16T17:28:26.129449Z","shell.execute_reply":"2025-02-16T17:28:31.020437Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50cf9a15e4d64c2a97407b4ba9c66ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80f0047e8a84881a03a73f3a3fc6732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b099221c04d4c2f94f7020063b67eae"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:28:31.022651Z","iopub.execute_input":"2025-02-16T17:28:31.022916Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 800 | Num Epochs = 5\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 16 | Total steps = 250\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  8/250 01:53 < 1:16:27, 0.05 it/s, Epoch 0.14/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Inference Test\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nexample = train_dataset[0]\ninputs = tokenizer(\n[\n    alpaca_prompt.format(map_dict[example['language']], example['sentence'], '')\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nresult = tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0]\nresult, example['label']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ndef get_results(examples):\n    results = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(map_dict[langs], sentences, '')\n        ], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    result = int(tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0])\n    return {'result': result}\n\ntrain_dataset = train_dataset.map(get_results)\nvalid_dataset = valid_dataset.map(get_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_F1(actual, predicted):\n    tp, tn, fp, fn = 0, 0, 0, 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            if actual[i] == 1: tp += 1\n            else: tn += 1\n        else:\n            if actual[i] == 1: fn += 1\n            else: fp += 1\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    accuracy = (tp+tn) / (tp+tn+fp+fn)\n    f1_score = (2 * precision * recall) / (precision+recall)\n    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')\n    print(f'F1 Score: {f1_score}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Train Metrics')\ncalculate_F1(train_dataset[:]['label'], train_dataset[:]['result'])\nprint('Validation Metrics')\ncalculate_F1(valid_dataset[:]['label'], valid_dataset[:]['result'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files = [test_file_path])[\"train\"]\ntest_dataset = test_dataset.map(get_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = test_dataset[:]['result']\npredictions = ['Positive' if predictions[i] == 1 else 'Negative' for i in range(len(predictions))]\nlen(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dl = pd.DataFrame({'ID': list(range(1,len(predictions)+1)), 'label': predictions})\ndl.to_csv('output.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}